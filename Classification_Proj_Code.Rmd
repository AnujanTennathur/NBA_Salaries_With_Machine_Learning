---
title: "Classification_Project_Code_NBA"
author: "AJ Tennathur"
date: "2025-04-01"
output:
  pdf_document: default
  html_document: default
---

``` {r}
library(knitr)
library(tidyverse)
library(moderndive)
library(caret)
library(mgcv)
library(rpart)
library(rpart.plot)
library(GGally)
library(ipred)
library(vip)
library(randomForest)
library(xgboost)
```

# Data Wrangling/Viz

``` {r}
ggpairs(salary, aes(color = Position), columns = c(3,4,5))
```

# Background

-- We are now looking at classification machine learning models for our 2022-2023 
NBA dataset. Later, we will also examine the earlier 2002-2003 NBA season, which we also 
do for our Regression Modeling. 
-- Our goal is to use machine learning classification models to predict player position
(Point Guard, Shooting Guard, Center, Etc.) based on various explanatory variables.

# KNN 

-- As part of our classification analysis, we will start with our 
K-Nearest Neighbors, or KNN, analysis. 
-- Important note: higher k values may lead to under fitting with more of a bias
towards general trends and relative resistance, while very small k values are prone to over fitting and being very sensitive to outlying data.
 

# Standardize
``` {r}
# use scale to standardize
salary_2=salary
salary_2[,25:31] =scale(salary_2[,25:31])
salary_2=data.frame(salary_2)

salary_2$Position = as.factor(salary_2$Position)

# now check the standardized variance
sd(salary_2[ ,25])
sd(salary_2[ ,31])
```

# Visualize

``` {r}
ggpairs(salary_2, aes(color = Position), columns = c(25,26,27,28,29, 30, 31))
```

``` {r}
ggplot(data = salary_2, aes(x = AST, y = TRB, col = Position)) + 
  geom_point(alpha=0.3)
```

``` {r}
set.seed(239)

PG= salary_2%>%
  filter(Position=="PG")

SG= salary_2%>%
  filter(Position=="SG")

SF= salary_2%>%
  filter(Position=="SF")

PF= salary_2%>%
  filter(Position=="PF")

C= salary_2%>%
  filter(Position=="C")

# 50 observations from each
# Take 60% to train (30) and 40% to test (20)
trainnba = sample(1:50, 30)

nba.train= rbind(PG[trainnba,], SG[trainnba,], SF[trainnba,], PF[trainnba,], C[trainnba,])
nba.test= rbind(PG[-trainnba,], SG[-trainnba,], SF[-trainnba,], 
                   PF[-trainnba,], C[-trainnba,])
```


``` {r}
library(class)
knnNBA= knn(train = nba.train[,25:31], 
              test = nba.test[,25:31], 
              cl = nba.train$Position, 
              k = 16)
## correct
mean(knnNBA==nba.test$Position)
mean(knnNBA!=nba.test$Position)
```

# Cross Validation

``` {r}
error = rep(0,20)
for (i in 1:20) {
  knnNBA= knn(train = nba.train[,25:31], test = nba.test[,25:31], cl = nba.train$Position, k = i)
  error[i] = 1- mean(knnNBA == nba.test$Position)
}

plot_data = data.frame(k=1:20,error)

ggplot(data = plot_data, aes(x = k, y = error)) +
  geom_line(color = "blue")+
  xlab("Neighborhood Size")
```

``` {r}
best_k = which.min(error)
best_k
```

# Prediction with best k

``` {r}
nba_pred = knn(train = nba.train[,25:31], 
                 test = nba.test[,25:31], 
                 cl = nba.train$Position, 
                 k=19)

table(nba.test$Position, nba_pred)

mean(nba_pred==nba.test$Position)

```

# KNN Model Conclusions



# Classification/Decision Trees

-- Next, we will do classification machine learning modeling through classification/decision trees. 

``` {r}
library(caret)

caretSampnba_2 = createDataPartition(salary_2$Position,
                                     p = 0.7, 
                                     list = FALSE)

traincaretnba_2 = salary_2[caretSampnba_2,]
testcaretnba_2 = salary_2[-caretSampnba_2,]

traincaretnba_2 = traincaretnba_2 %>%
  select(
    "Position", "TRB", "AST", "STL", "BLK", "PTS", "TOV", "PER", "OBPM", "FT", "FTA", 
    "X2P", "X2PA", "X3P", "X3PA", 
  )

testcaretnba_2 = testcaretnba_2 %>%
  select(
    "Position", "TRB", "AST", "STL", "BLK", "PTS", "TOV", "PER", "OBPM", "FT", "FTA", 
    "X2P", "X2PA", "X3P", "X3PA", 
  )

```

``` {r}
library(rpart)

classtree = rpart(Position ~ ., 
                  data = traincaretnba_2, 
                  method = "class")
rpart.plot(classtree)

```

```{r}
plotcp(classtree)
printcp(classtree)
```

``` {r}
minCP=classtree$cptable[which.min(classtree$cptable[,"xerror"]),"CP"]
minCP
```

  # Prune Tree

``` {r}
prune_classtree = prune(classtree, cp = minCP )
rpart.plot(prune_classtree)
```

  # Classification Trees -- Predictions 

# Default Tree
``` {r}
predTree1=predict(classtree, testcaretnba_2, type="class")

cmTree1=table(testcaretnba_2$Position, predTree1)
cmTree1

mean(testcaretnba_2$Position==predTree1)
```
# Prune Tree

``` {r}
predTree2=predict(prune_classtree, testcaretnba_2, type="class")

### CONFUSION MATRIX
cmTree2=table(testcaretnba_2$Position, predTree2)
cmTree2

#### CORRECT RATE
mean(testcaretnba_2$Position==predTree2)
```

# Classification Tree(s) Conclusions



# Bagging
  -- While classification decision trees give us a general view of our model, 
  another method that is effective is Bagging, which stands for 
  “bootstrap aggregation”. 
  -- Random bootstrap samples (with replacement) of the data are used to create trees.
  -- It then aggregates these different trees (that are based on different subset samples) to create a final model. 
  -- The big idea is that averaging a set of observations reduces variance.

-- ipred library

``` {r}
### BAG
set.seed(252)
pimaBag = bagging(Position ~ .,
                   data = traincaretnba_2,
                   nbagg = 150,   
                   coob = TRUE,
                   control = rpart.control(minsplit = 2, cp = 0))

## PREDICT
predBag=predict(pimaBag, testcaretnba_2, type="class")

## CONFUSION MATRIX
cmBag=table(testcaretnba_2$Position, predBag)
cmBag

mean(testcaretnba_2$Position==predBag) # correct rate
```
# Repeat with Caret

``` {r}
set.seed(252)
caretBag = train(Position ~., 
               data = traincaretnba_2, 
               method = "treebag",
               trControl = trainControl("cv", number = 10),
               importance = TRUE
)

predCaretBag = caretBag %>% predict(testcaretnba_2)

table(predCaretBag, testcaretnba_2$Position)

mean(predCaretBag == testcaretnba_2$Position) # Correct Rate

```


# Bag Variable Importance: Let's see which variables are having the largest 
"influence" on our model

``` {r}
vip(caretBag)
```

# Bagging Model Conclusions




# Random Forests

There are some problems with Bagging: 
- Trees can be very similar
- Dominated by a few strong / moderately strong predictor
- Bagged trees can be highly correlated
- Does not lead to large reduction in variance when averaging


-- We can further improve on bagging through random forests modeling. 

``` {r}
set.seed(250)

caretRF = train(Position ~., 
                data = traincaretnba_2, 
                method = "rf", 
                trControl = trainControl("cv", number = 10),
                importance = TRUE
          )
caretRF$bestTune
caretRF$finalModel
```
# predictions with RF

``` {r}
predCaretRF = caretRF %>% predict(testcaretnba_2)
table(predCaretRF, testcaretnba_2$Position)

mean(predCaretRF==testcaretnba_2$Position)
```

# Variable Importance: Let's see which variables are contributing to "splits" and 
which variables are reducing gini (impurity). 

``` {r}
varImpPlot(caretRF$finalModel, type = 1)
varImpPlot(caretRF$finalModel, type = 2)
```

# Random Forests Conclusions


# Boosting
-- Boosting is essentially growing the decision trees sequentially. 
-- The Model learns slowly (updates based on some scaled version of the previous tree). -- It tends to perform well, with each tree growing using information from previously grown trees. 


``` {r}
caretBoost = train(Position ~., 
                  data = traincaretnba_2, 
                  method = "xgbTree", 
                  trControl = trainControl("cv", number = 10),
                  importance = TRUE
          )

caretBoost$bestTune

```

``` {r}
predCaretBoost = caretBoost %>% predict(testcaretnba_2)
table(predCaretBoost, testcaretnba_2$Position)

mean(predCaretBoost == testcaretnba_2$Position)
```


# Boosting Conclusions


# Overall Conclusions



-- End of Models -- 